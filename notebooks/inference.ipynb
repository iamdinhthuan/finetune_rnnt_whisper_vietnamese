{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import pipeline\n",
    "\n",
    "from models.encoder import AudioEncoder\n",
    "from models.decoder import Decoder\n",
    "from models.jointer import Jointer\n",
    "\n",
    "from constants import SAMPLE_RATE, N_FFT, HOP_LENGTH, N_MELS\n",
    "from constants import RNNT_BLANK, PAD, VOCAB_SIZE, TOKENIZER_MODEL_PATH, MAX_SYMBOLS\n",
    "from constants import ATTENTION_CONTEXT_SIZE\n",
    "from constants import N_STATE, N_LAYER, N_HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is FP32\n",
    "trained_model_path = hf_hub_download(\n",
    "    repo_id=\"hkab/vietnamese-asr-model\", \n",
    "    filename=\"rnnt-latest.ckpt\",\n",
    "    subfolder=\"rnnt-whisper-small/80_3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(trained_model_path, map_location=\"cpu\", weights_only=True )\n",
    "\n",
    "encoder_weight = {}\n",
    "decoder_weight = {}\n",
    "joint_weight = {}\n",
    "\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if 'alibi' in k:\n",
    "        continue\n",
    "    if 'encoder' in k:\n",
    "        encoder_weight[k.replace('encoder.', '')] = v\n",
    "    elif 'decoder' in k:\n",
    "        decoder_weight[k.replace('decoder.', '')] = v\n",
    "    elif 'joint' in k:\n",
    "        joint_weight[k.replace('joint.', '')] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AudioEncoder(\n",
    "    N_MELS,\n",
    "    n_state=N_STATE,\n",
    "    n_head=N_HEAD,\n",
    "    n_layer=N_LAYER,\n",
    "    att_context_size=ATTENTION_CONTEXT_SIZE\n",
    ")\n",
    "\n",
    "decoder = Decoder(vocab_size=VOCAB_SIZE + 1)\n",
    "\n",
    "joint = Jointer(vocab_size=VOCAB_SIZE + 1)\n",
    "\n",
    "encoder.load_state_dict(encoder_weight, strict=False)\n",
    "decoder.load_state_dict(decoder_weight, strict=False)\n",
    "joint.load_state_dict(joint_weight, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "joint = joint.to(DEVICE)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "joint.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_filters(device, n_mels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    load the mel filterbank matrix for projecting STFT into a Mel spectrogram.\n",
    "    Allows decoupling librosa dependency; saved using:\n",
    "\n",
    "        np.savez_compressed(\n",
    "            \"mel_filters.npz\",\n",
    "            mel_80=librosa.filters.mel(sr=SAMPLE_RATE, n_fft=400, n_mels=80),\n",
    "            mel_128=librosa.filters.mel(sr=SAMPLE_RATE, n_fft=400, n_mels=128),\n",
    "        )\n",
    "    \"\"\"\n",
    "    assert n_mels in {80, 128}, f\"Unsupported n_mels: {n_mels}\"\n",
    "\n",
    "    with np.load(\"./utils/mel_filters.npz\", allow_pickle=False) as f:\n",
    "        return torch.from_numpy(f[f\"mel_{n_mels}\"]).to(device)\n",
    "\n",
    "def log_mel_spectrogram(\n",
    "    audio, n_mels, padding, streaming, device\n",
    "):\n",
    "\n",
    "    if device is not None:\n",
    "        audio = audio.to(device)\n",
    "    if padding > 0:\n",
    "        audio = F.pad(audio, (0, padding))\n",
    "    window = torch.hann_window(N_FFT).to(audio.device)\n",
    "    if not streaming:\n",
    "        stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n",
    "    else:\n",
    "        stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, center=False, return_complex=True)\n",
    "    magnitudes = stft[..., :-1].abs() ** 2\n",
    "\n",
    "    filters = mel_filters(audio.device, n_mels)\n",
    "    mel_spec = filters @ magnitudes\n",
    "\n",
    "    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n",
    "    # log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n",
    "    log_spec = (log_spec + 4.0) / 4.0\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online and offline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offline_transcribe(audio, encoder, decoder, joint, tokenizer, max_symbols=3):\n",
    "    \n",
    "    mels = log_mel_spectrogram(audio=audio, n_mels=N_MELS, padding=0, streaming=False, device=\"cpu\")\n",
    "    x = mels.reshape(1, *mels.shape).to(DEVICE)\n",
    "    x_len = torch.tensor([x.shape[2]]).to(DEVICE)\n",
    "\n",
    "    enc_out, _ = encoder(x, x_len)\n",
    "\n",
    "    all_sentences = []\n",
    "    # greedy decoding, handle each sequence independently for easier implementation\n",
    "    for batch_idx in range(enc_out.shape[0]):\n",
    "        hypothesis = [[None, None]]  # [label, state]\n",
    "        seq_enc_out = enc_out[batch_idx, :, :].unsqueeze(0) # [1, T, D]\n",
    "        seq_ids = []\n",
    "        \n",
    "        for time_idx in range(seq_enc_out.shape[1]):\n",
    "            curent_seq_enc_out = seq_enc_out[:, time_idx, :].unsqueeze(1) # 1, 1, D\n",
    "\n",
    "            not_blank = True\n",
    "            symbols_added = 0\n",
    "\n",
    "            while not_blank and symbols_added < max_symbols:\n",
    "                # In the first timestep, we initialize the network with RNNT Blank\n",
    "                # In later timesteps, we provide previous predicted label as input.\n",
    "                if hypothesis[-1][0] is None:\n",
    "                    last_token = torch.tensor([[RNNT_BLANK]], dtype=torch.long, device=seq_enc_out.device)\n",
    "                    last_seq_h_n = None\n",
    "                else:\n",
    "                    last_token = hypothesis[-1][0]\n",
    "                    last_seq_h_n = hypothesis[-1][1]\n",
    "                \n",
    "                if last_seq_h_n is None:\n",
    "                    current_seq_dec_out, current_seq_h_n = decoder(last_token)\n",
    "                else:\n",
    "                    current_seq_dec_out, current_seq_h_n = decoder(last_token, last_seq_h_n)\n",
    "                logits = joint(curent_seq_enc_out, current_seq_dec_out)[0, 0, 0, :]  # (B, T=1, U=1, V + 1)\n",
    "\n",
    "                del current_seq_dec_out\n",
    "\n",
    "                _, token_id = logits.max(0)\n",
    "                token_id = token_id.detach().item()  # K is the label at timestep t_s in inner loop, s >= 0.\n",
    "\n",
    "                del logits\n",
    "\n",
    "                if token_id == RNNT_BLANK:\n",
    "                    not_blank = False\n",
    "                else:\n",
    "                    symbols_added += 1\n",
    "                    hypothesis.append([\n",
    "                        torch.tensor([[token_id]], dtype=torch.long, device=curent_seq_enc_out.device),\n",
    "                        current_seq_h_n\n",
    "                    ])\n",
    "                    seq_ids.append(token_id)\n",
    "        all_sentences.append(tokenizer.decode(seq_ids))\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = librosa.load(\"/path/to/audio.wav\", sr=SAMPLE_RATE)[0]\n",
    "audio = torch.from_numpy(audio).to(DEVICE)\n",
    "offline_transcribe(audio, encoder, decoder, joint, tokenizer, max_symbols=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_transcribe(audio, encoder, decoder, joint, tokenizer, max_symbols=3):\n",
    "    audio_cache = torch.zeros(240, device=DEVICE) # audio[:240]\n",
    "\n",
    "    conv1_cache = torch.zeros(1, 80, 1, device=DEVICE)\n",
    "    conv2_cache = torch.zeros(1, 768, 1, device=DEVICE)\n",
    "    conv3_cache = torch.zeros(1, 768, 1, device=DEVICE)\n",
    "\n",
    "    k_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=DEVICE)\n",
    "    v_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=DEVICE)\n",
    "    cache_len = torch.zeros(1, dtype=torch.int, device=DEVICE)\n",
    "\n",
    "    hypothesis = [[None, None]]  # [label, state]\n",
    "    seq_ids = []\n",
    "\n",
    "    for i in tqdm(range(240, audio.shape[0], HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH))):\n",
    "        audio_chunk = torch.cat([audio_cache, audio[i:i+HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH)]])\n",
    "        # print(f\"From {i - 240} to {i+HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH)}\")\n",
    "        if audio_chunk.shape[0] < HOP_LENGTH * 31 + N_FFT:\n",
    "            audio_chunk = F.pad(audio_chunk, (0, HOP_LENGTH * 31 + N_FFT - audio_chunk.shape[0]))\n",
    "        audio_cache = audio_chunk[-(N_FFT - HOP_LENGTH):]\n",
    "        x_chunk = log_mel_spectrogram(audio=audio_chunk, n_mels=N_MELS, padding=0, streaming=True, device=\"cuda\")\n",
    "        x_chunk = x_chunk.reshape(1, *x_chunk.shape)\n",
    "\n",
    "        if x_chunk.shape[-1] < 32:\n",
    "            x_chunk = F.pad(x_chunk, (0, 32 - x_chunk.shape[-1]))\n",
    "        x_chunk = torch.cat([conv1_cache, x_chunk], dim=2)\n",
    "\n",
    "        conv1_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(encoder.conv1(x_chunk))\n",
    "\n",
    "        x_chunk = torch.cat([conv2_cache, x_chunk], dim=2)\n",
    "        conv2_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(encoder.conv2(x_chunk))\n",
    "        \n",
    "        x_chunk = torch.cat([conv3_cache, x_chunk], dim=2)\n",
    "        conv3_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(encoder.conv3(x_chunk))\n",
    "\n",
    "        x_chunk = x_chunk.permute(0, 2, 1)\n",
    "\n",
    "        x_len = torch.tensor([x_chunk.shape[1]]).to(DEVICE)\n",
    "        if k_cache is not None:\n",
    "            x_len = x_len + ATTENTION_CONTEXT_SIZE[0]\n",
    "            offset = torch.neg(cache_len) + ATTENTION_CONTEXT_SIZE[0]\n",
    "        else:\n",
    "            offset = None\n",
    "\n",
    "        attn_mask = encoder.form_attention_mask_for_streaming(encoder.att_context_size, x_len, offset.to(DEVICE), DEVICE)\n",
    "\n",
    "        if k_cache is not None:\n",
    "            attn_mask = attn_mask[:, :, ATTENTION_CONTEXT_SIZE[0]:, :]\n",
    "\n",
    "        new_k_cache = []\n",
    "        new_v_cache = []\n",
    "        for i, block in enumerate(encoder.blocks):\n",
    "            x_chunk, layer_k_cache, layer_v_cache = block(x_chunk, mask=attn_mask, k_cache=k_cache[i], v_cache=v_cache[i])\n",
    "            new_k_cache.append(layer_k_cache)\n",
    "            new_v_cache.append(layer_v_cache)\n",
    "\n",
    "        enc_out = encoder.ln_post(x_chunk)\n",
    "\n",
    "        k_cache = torch.stack(new_k_cache, dim=0)\n",
    "        v_cache = torch.stack(new_v_cache, dim=0)\n",
    "        cache_len = torch.clamp(cache_len + ATTENTION_CONTEXT_SIZE[-1] + 1, max=ATTENTION_CONTEXT_SIZE[0])\n",
    "\n",
    "        # Greedy decoding\n",
    "        seq_enc_out = enc_out[0, :, :].unsqueeze(0) # [1, T, D]\n",
    "        \n",
    "        for time_idx in range(seq_enc_out.shape[1]):\n",
    "            curent_seq_enc_out = seq_enc_out[:, time_idx, :].unsqueeze(1) # 1, 1, D\n",
    "\n",
    "            not_blank = True\n",
    "            symbols_added = 0\n",
    "\n",
    "            while not_blank and symbols_added < max_symbols:\n",
    "                # In the first timestep, we initialize the network with RNNT Blank\n",
    "                # In later timesteps, we provide previous predicted label as input.\n",
    "                if hypothesis[-1][0] is None:\n",
    "                    last_token = torch.tensor([[RNNT_BLANK]], dtype=torch.long, device=seq_enc_out.device)\n",
    "                    last_seq_h_n = None\n",
    "                else:\n",
    "                    last_token = hypothesis[-1][0]\n",
    "                    last_seq_h_n = hypothesis[-1][1]\n",
    "                \n",
    "                if last_seq_h_n is None:\n",
    "                    current_seq_dec_out, current_seq_h_n = decoder(last_token)\n",
    "                else:\n",
    "                    current_seq_dec_out, current_seq_h_n = decoder(last_token, last_seq_h_n)\n",
    "                logits = joint(curent_seq_enc_out, current_seq_dec_out)[0, 0, 0, :]  # (B, T=1, U=1, V + 1)\n",
    "\n",
    "                del current_seq_dec_out\n",
    "\n",
    "                _, token_id = logits.max(0)\n",
    "                token_id = token_id.detach().item()  # K is the label at timestep t_s in inner loop, s >= 0.\n",
    "\n",
    "                del logits\n",
    "\n",
    "                if token_id == RNNT_BLANK:\n",
    "                    not_blank = False\n",
    "                else:\n",
    "                    symbols_added += 1\n",
    "                    hypothesis.append([\n",
    "                        torch.tensor([[token_id]], dtype=torch.long, device=curent_seq_enc_out.device),\n",
    "                        current_seq_h_n\n",
    "                    ])\n",
    "                    seq_ids.append(token_id)\n",
    "    return tokenizer.decode(seq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = librosa.load(\"/path/to/audio.wav\", sr=SAMPLE_RATE)[0]\n",
    "audio = torch.from_numpy(audio).to(DEVICE)\n",
    "online_transcribe(audio, encoder, decoder, joint, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

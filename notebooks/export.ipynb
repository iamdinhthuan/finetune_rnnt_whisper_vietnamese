{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fft import rfft as fft\n",
    "from scipy.signal import check_COLA, get_window\n",
    "import torch.nn as nn\n",
    "\n",
    "support_clp_op = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sentencepiece as spm\n",
    "import librosa\n",
    "import numpy as np\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "from models.encoder import AudioEncoder\n",
    "from models.decoder import Decoder\n",
    "from models.jointer import Jointer\n",
    "\n",
    "from constants import SAMPLE_RATE, N_FFT, HOP_LENGTH, N_MELS\n",
    "from constants import RNNT_BLANK, PAD, VOCAB_SIZE, TOKENIZER_MODEL_PATH, MAX_SYMBOLS\n",
    "from constants import ATTENTION_CONTEXT_SIZE\n",
    "from constants import N_STATE, N_LAYER, N_HEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model is FP32\n",
    "trained_model_path = hf_hub_download(\n",
    "    repo_id=\"hkab/vietnamese-asr-model\", \n",
    "    filename=\"rnnt-latest.ckpt\",\n",
    "    subfolder=\"rnnt-whisper-small/80_3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(trained_model_path, map_location=\"cpu\", weights_only=True )\n",
    "\n",
    "encoder_weight = {}\n",
    "decoder_weight = {}\n",
    "joint_weight = {}\n",
    "\n",
    "for k, v in checkpoint['state_dict'].items():\n",
    "    if 'alibi' in k:\n",
    "        continue\n",
    "    if 'encoder' in k:\n",
    "        encoder_weight[k.replace('encoder.', '')] = v\n",
    "    elif 'decoder' in k:\n",
    "        decoder_weight[k.replace('decoder.', '')] = v\n",
    "    elif 'joint' in k:\n",
    "        joint_weight[k.replace('joint.', '')] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = AudioEncoder(\n",
    "    n_mels=N_MELS,\n",
    "    n_state=N_STATE,\n",
    "    n_head=N_HEAD,\n",
    "    n_layer=N_LAYER,\n",
    "    att_context_size=ATTENTION_CONTEXT_SIZE\n",
    ")\n",
    "\n",
    "decoder = Decoder(vocab_size=VOCAB_SIZE + 1)\n",
    "\n",
    "joint = Jointer(vocab_size=VOCAB_SIZE + 1)\n",
    "\n",
    "encoder.load_state_dict(encoder_weight, strict=False)\n",
    "decoder.load_state_dict(decoder_weight, strict=False)\n",
    "joint.load_state_dict(joint_weight, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "joint = joint.to(DEVICE)\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "joint.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportable STFT from https://github.com/echocatzh/conv-stft/blob/master/conv_stft/conv_stft.py\n",
    "class STFT(torch.nn.Module):\n",
    "    def __init__(self, win_len=1024, win_hop=512, fft_len=1024,\n",
    "                 enframe_mode='continue', win_type='hann',\n",
    "                 win_sqrt=False, pad_center=True):\n",
    "        \"\"\"\n",
    "        Implement of STFT using 1D convolution and 1D transpose convolutions.\n",
    "        Implement of framing the signal in 2 ways, `break` and `continue`.\n",
    "        `break` method is a kaldi-like framing.\n",
    "        `continue` method is a librosa-like framing.\n",
    "\n",
    "        More information about `perfect reconstruction`:\n",
    "        1. https://ww2.mathworks.cn/help/signal/ref/stft.html\n",
    "        2. https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.get_window.html\n",
    "\n",
    "        Args:\n",
    "            win_len (int): Number of points in one frame.  Defaults to 1024.\n",
    "            win_hop (int): Number of framing stride. Defaults to 512.\n",
    "            fft_len (int): Number of DFT points. Defaults to 1024.\n",
    "            enframe_mode (str, optional): `break` and `continue`. Defaults to 'continue'.\n",
    "            win_type (str, optional): The type of window to create. Defaults to 'hann'.\n",
    "            win_sqrt (bool, optional): using square root window. Defaults to True.\n",
    "            pad_center (bool, optional): `perfect reconstruction` opts. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(STFT, self).__init__()\n",
    "        assert enframe_mode in ['break', 'continue']\n",
    "        assert fft_len >= win_len\n",
    "        self.win_len = win_len\n",
    "        self.win_hop = win_hop\n",
    "        self.fft_len = fft_len\n",
    "        self.mode = enframe_mode\n",
    "        self.win_type = win_type\n",
    "        self.win_sqrt = win_sqrt\n",
    "        self.pad_center = pad_center\n",
    "        self.pad_amount = self.fft_len // 2\n",
    "\n",
    "        en_k, fft_k, ifft_k, ola_k = self.__init_kernel__()\n",
    "        self.register_buffer('en_k', en_k)\n",
    "        self.register_buffer('fft_k', fft_k)\n",
    "        self.register_buffer('ifft_k', ifft_k)\n",
    "        self.register_buffer('ola_k', ola_k)\n",
    "\n",
    "    def __init_kernel__(self):\n",
    "        \"\"\"\n",
    "        Generate enframe_kernel, fft_kernel, ifft_kernel and overlap-add kernel.\n",
    "        ** enframe_kernel: Using conv1d layer and identity matrix.\n",
    "        ** fft_kernel: Using linear layer for matrix multiplication. In fact,\n",
    "        enframe_kernel and fft_kernel can be combined, But for the sake of \n",
    "        readability, I took the two apart.\n",
    "        ** ifft_kernel, pinv of fft_kernel.\n",
    "        ** overlap-add kernel, just like enframe_kernel, but transposed.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: four kernels.\n",
    "        \"\"\"\n",
    "        enframed_kernel = torch.eye(self.fft_len)[:, None, :]\n",
    "        if support_clp_op:\n",
    "            tmp = fft(torch.eye(self.fft_len))\n",
    "            fft_kernel = torch.stack([tmp.real, tmp.imag], dim=2)\n",
    "        else:\n",
    "            fft_kernel = fft(torch.eye(self.fft_len), 1)\n",
    "        if self.mode == 'break':\n",
    "            enframed_kernel = torch.eye(self.win_len)[:, None, :]\n",
    "            fft_kernel = fft_kernel[:self.win_len]\n",
    "        fft_kernel = torch.cat(\n",
    "            (fft_kernel[:, :, 0], fft_kernel[:, :, 1]), dim=1)\n",
    "        ifft_kernel = torch.pinverse(fft_kernel)[:, None, :]\n",
    "        window = get_window(self.win_type, self.win_len, fftbins=False)\n",
    "\n",
    "        self.perfect_reconstruct = check_COLA(\n",
    "            window,\n",
    "            self.win_len,\n",
    "            self.win_len-self.win_hop)\n",
    "        window = torch.FloatTensor(window)\n",
    "        if self.mode == 'continue':\n",
    "            left_pad = (self.fft_len - self.win_len)//2\n",
    "            right_pad = left_pad + (self.fft_len - self.win_len) % 2\n",
    "            window = F.pad(window, (left_pad, right_pad))\n",
    "        if self.win_sqrt:\n",
    "            self.padded_window = window\n",
    "            window = torch.sqrt(window)\n",
    "        else:\n",
    "            self.padded_window = window**2\n",
    "\n",
    "        fft_kernel = fft_kernel.T * window\n",
    "        ifft_kernel = ifft_kernel * window\n",
    "        ola_kernel = torch.eye(self.fft_len)[:self.win_len, None, :]\n",
    "        if self.mode == 'continue':\n",
    "            ola_kernel = torch.eye(self.fft_len)[:, None, :self.fft_len]\n",
    "        return enframed_kernel, fft_kernel, ifft_kernel, ola_kernel\n",
    "\n",
    "    def is_perfect(self):\n",
    "        \"\"\"\n",
    "        Whether the parameters win_len, win_hop and win_sqrt\n",
    "        obey constants overlap-add(COLA)\n",
    "\n",
    "        Returns:\n",
    "            bool: Return true if parameters obey COLA.\n",
    "        \"\"\"\n",
    "        return self.perfect_reconstruct and self.pad_center\n",
    "\n",
    "    def transform(self, inputs, return_type='complex'):\n",
    "        \"\"\"Take input data (audio) to STFT domain.\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): Tensor of floats, with shape (num_batch, num_samples)\n",
    "            return_type (str, optional): return (mag, phase) when `magphase`,\n",
    "            return (real, imag) when `realimag` and complex(real, imag) when `complex`.\n",
    "            Defaults to 'complex'.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (mag, phase) when `magphase`, return (real, imag) when\n",
    "            `realimag`. Defaults to 'complex', each elements with shape \n",
    "            [num_batch, num_frequencies, num_frames]\n",
    "        \"\"\"\n",
    "        assert return_type in ['magphase', 'realimag', 'complex']\n",
    "        if inputs.dim() == 2:\n",
    "            inputs = torch.unsqueeze(inputs, 1)\n",
    "        self.num_samples = inputs.size(-1)\n",
    "        if self.pad_center:\n",
    "            inputs = F.pad(\n",
    "                inputs, (self.pad_amount, self.pad_amount), mode='reflect')\n",
    "        enframe_inputs = F.conv1d(inputs, self.en_k, stride=self.win_hop)\n",
    "        outputs = torch.transpose(enframe_inputs, 1, 2)\n",
    "        outputs = F.linear(outputs, self.fft_k)\n",
    "        outputs = torch.transpose(outputs, 1, 2)\n",
    "        dim = self.fft_len//2+1\n",
    "        real = outputs[:, :dim, :]\n",
    "        imag = outputs[:, dim:, :]\n",
    "        if return_type == 'realimag':\n",
    "            return real, imag\n",
    "        elif return_type == 'complex':\n",
    "            assert support_clp_op\n",
    "            return torch.complex(real, imag)\n",
    "        else:\n",
    "            mags = torch.sqrt(real**2+imag**2)\n",
    "            phase = torch.atan2(imag, real)\n",
    "            return mags, phase\n",
    "\n",
    "    def inverse(self, input1, input2=None, input_type='magphase'):\n",
    "        \"\"\"Call the inverse STFT (iSTFT), given tensors produced \n",
    "        by the `transform` function.\n",
    "\n",
    "        Args:\n",
    "            input1 (tensors): Magnitude/Real-part of STFT with shape \n",
    "            [num_batch, num_frequencies, num_frames]\n",
    "            input2 (tensors): Phase/Imag-part of STFT with shape\n",
    "            [num_batch, num_frequencies, num_frames]\n",
    "            input_type (str, optional): Mathematical meaning of input tensor's.\n",
    "            Defaults to 'magphase'.\n",
    "\n",
    "        Returns:\n",
    "            tensors: Reconstructed audio given magnitude and phase. Of\n",
    "                shape [num_batch, num_samples]\n",
    "        \"\"\"\n",
    "        assert input_type in ['magphase', 'realimag']\n",
    "        if input_type == 'realimag':\n",
    "            real, imag = None, None\n",
    "            if support_clp_op and torch.is_complex(input1):\n",
    "                real, imag = input1.real, input1.imag\n",
    "            else:\n",
    "                real, imag = input1, input2\n",
    "        else:\n",
    "            real = input1*torch.cos(input2)\n",
    "            imag = input1*torch.sin(input2)\n",
    "        inputs = torch.cat([real, imag], dim=1)\n",
    "        outputs = F.conv_transpose1d(inputs, self.ifft_k, stride=self.win_hop)\n",
    "        t = (self.padded_window[None, :, None]).repeat(1, 1, inputs.size(-1))\n",
    "        t = t.to(inputs.device)\n",
    "        coff = F.conv_transpose1d(t, self.ola_k, stride=self.win_hop)\n",
    "        rm_start, rm_end = self.pad_amount, self.pad_amount+self.num_samples\n",
    "        outputs = outputs[..., rm_start:rm_end]\n",
    "        coff = coff[..., rm_start:rm_end]\n",
    "        coffidx = torch.where(coff > 1e-8)\n",
    "        outputs[coffidx] = outputs[coffidx]/(coff[coffidx])\n",
    "        return outputs.squeeze(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Take input data (audio) to STFT domain and then back to audio.\n",
    "\n",
    "        Args:\n",
    "            inputs (tensor): Tensor of floats, with shape [num_batch, num_samples]\n",
    "\n",
    "        Returns:\n",
    "            tensor: Reconstructed audio given magnitude and phase.\n",
    "            Of shape [num_batch, num_samples]\n",
    "        \"\"\"\n",
    "        mag, phase = self.transform(inputs)\n",
    "        rec_wav = self.inverse(mag, phase)\n",
    "        return rec_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperPreprocessor(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.stft = STFT(\n",
    "            win_len=400, win_hop=160, fft_len=400,\n",
    "\t\t\tpad_center=False # For streaming\n",
    "        )\n",
    "\t\t# self.filters = torch.from_numpy(librosa.filters.mel(sr=16000, n_fft=400, n_mels=80))\n",
    "\n",
    "\t\tself.register_buffer('filters', torch.from_numpy(librosa.filters.mel(sr=SAMPLE_RATE, n_fft=N_FFT, n_mels=N_MELS)))\n",
    "\t\n",
    "\tdef forward(self, audio_signal):\n",
    "\t\tmags, _ = self.stft.transform(audio_signal, return_type='magphase')\n",
    "\t\tmags = mags**2\n",
    "\n",
    "\t\taudio_signal = self.filters @ mags\n",
    "\t\taudio_signal = torch.clamp(audio_signal, min=1e-10).log10()\n",
    "\t\taudio_signal = (audio_signal + 4.0) / 4.0\n",
    "\n",
    "\t\treturn audio_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperEncoderALiBi(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder.to(DEVICE)\n",
    "        self.preprocessor = WrapperPreprocessor().to(DEVICE)\n",
    "\n",
    "    def forward(self, \n",
    "                audio_chunk, audio_cache, \n",
    "                conv1_cache, conv2_cache, conv3_cache,\n",
    "                k_cache, v_cache, cache_len):\n",
    "        audio_chunk = torch.cat([audio_cache, audio_chunk], dim=1)\n",
    "        audio_cache = audio_chunk[:, -(N_FFT - HOP_LENGTH):]\n",
    "\n",
    "        x_chunk = self.preprocessor(audio_chunk)\n",
    "        x_chunk = torch.cat([conv1_cache, x_chunk], dim=2)\n",
    "\n",
    "        conv1_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(self.encoder.conv1(x_chunk))\n",
    "\n",
    "        x_chunk = torch.cat([conv2_cache, x_chunk], dim=2)\n",
    "        conv2_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(self.encoder.conv2(x_chunk))\n",
    "\n",
    "        x_chunk = torch.cat([conv3_cache, x_chunk], dim=2)\n",
    "        conv3_cache = x_chunk[:, :, -1].unsqueeze(2)\n",
    "        x_chunk = F.gelu(self.encoder.conv3(x_chunk))\n",
    "\n",
    "        x_chunk = x_chunk.permute(0, 2, 1)\n",
    "\n",
    "        x_len = torch.tensor([ATTENTION_CONTEXT_SIZE[0] + ATTENTION_CONTEXT_SIZE[1] + 1]).to(DEVICE)\n",
    "        offset = torch.neg(cache_len) + ATTENTION_CONTEXT_SIZE[0]\n",
    "\n",
    "        attn_mask = self.encoder.form_attention_mask_for_streaming(ATTENTION_CONTEXT_SIZE, x_len, offset.to(DEVICE), DEVICE)\n",
    "        attn_mask = attn_mask[:, :, ATTENTION_CONTEXT_SIZE[0]:, :]\n",
    "\n",
    "        new_k_cache = []\n",
    "        new_v_cache = []\n",
    "        for i, block in enumerate(self.encoder.blocks):\n",
    "            x_chunk, layer_k_cache, layer_v_cache = block(x_chunk, mask=attn_mask, k_cache=k_cache[i], v_cache=v_cache[i])\n",
    "            new_k_cache.append(layer_k_cache)\n",
    "            new_v_cache.append(layer_v_cache)\n",
    "\n",
    "        enc_out = self.encoder.ln_post(x_chunk)\n",
    "\n",
    "        k_cache = torch.stack(new_k_cache, dim=0)\n",
    "        v_cache = torch.stack(new_v_cache, dim=0)\n",
    "        cache_len = torch.clamp(cache_len + ATTENTION_CONTEXT_SIZE[1] + 1, max=ATTENTION_CONTEXT_SIZE[0])\n",
    "\n",
    "        return enc_out, audio_cache, conv1_cache, conv2_cache, conv3_cache, k_cache, v_cache, cache_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_encoder = WrapperEncoderALiBi(encoder)\n",
    "export_encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_chunk = torch.zeros(1, HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH), device=DEVICE)\n",
    "audio_cache = torch.zeros(1, N_FFT - HOP_LENGTH, device=DEVICE)\n",
    "conv1_cache = torch.zeros(1, 80, 1, device=DEVICE)\n",
    "conv2_cache = torch.zeros(1, 768, 1, device=DEVICE)\n",
    "conv3_cache = torch.zeros(1, 768, 1, device=DEVICE)\n",
    "\n",
    "k_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=DEVICE)\n",
    "v_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=DEVICE)\n",
    "cache_len = torch.zeros(1, dtype=torch.int, device=DEVICE)\n",
    "\n",
    "r = export_encoder(\n",
    "    audio_chunk, audio_cache, \n",
    "    conv1_cache, conv2_cache, conv3_cache, \n",
    "    k_cache, v_cache, cache_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        export_encoder,\n",
    "        (audio_chunk, audio_cache, conv1_cache, conv2_cache, conv3_cache, k_cache, v_cache, cache_len),\n",
    "        \"./onnx/encoder.onnx\",\n",
    "        input_names=[\"audio_chunk\", \"audio_cache\", \"conv1_cache\", \"conv2_cache\", \"conv3_cache\", \"k_cache\", \"v_cache\", \"cache_len\"],\n",
    "        output_names=[\"enc_out\", \"audio_cache\", \"conv1_cache\", \"conv2_cache\", \"conv3_cache\", \"k_cache\", \"v_cache\", \"cache_len\"],\n",
    "        export_params=True,\n",
    "\t\topset_version=17,\n",
    "\t\tdo_constant_folding=False, # Must be false for alibi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperDecoder(nn.Module):\n",
    "    def __init__(self, decoder):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.decoder = decoder.to(self.device)\n",
    "\n",
    "    def forward(self, token, h_n):\n",
    "        dec, h_n = self.decoder(token, h_n)\n",
    "        return dec, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = WrapperDecoder(decoder)\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = torch.tensor([[RNNT_BLANK]], dtype=torch.long, device=DEVICE)\n",
    "h_n = torch.zeros(1, 1, 768, device=DEVICE)\n",
    "\n",
    "r = decoder(token, h_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        decoder,\n",
    "        (token, h_n),\n",
    "        \"./onnx/decoder.onnx\",\n",
    "        input_names=[\"token\", \"h_n\"],\n",
    "        output_names=[\"dec\", \"h_n\"],\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperJoint(nn.Module):\n",
    "    def __init__(self, joint):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.joint = joint.to(self.device)\n",
    "\n",
    "    def forward(self, enc, dec):\n",
    "        return self.joint(enc, dec)[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jointer = WrapperJoint(joint)\n",
    "jointer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = torch.zeros(1, 1, 768, device=DEVICE)\n",
    "dec = torch.zeros(1, 1, 768, device=DEVICE)\n",
    "\n",
    "r = jointer(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    torch.onnx.export(\n",
    "        jointer,\n",
    "        (enc, dec),\n",
    "        \"./onnx/jointer.onnx\",\n",
    "        input_names=[\"enc\", \"dec\"],\n",
    "        output_names=[\"output\"],\n",
    "        export_params=True,\n",
    "        opset_version=17,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN: python -m onnxruntime.quantization.preprocess --input jointer.onnx --output jointer-infer.onnx before quantization\n",
    "import subprocess\n",
    "\n",
    "subprocess.run([\"python\", \"-m\", \"onnxruntime.quantization.preprocess\", \"--input\", \"./onnx/encoder.onnx\", \"--output\", \"./onnx/encoder-infer.onnx\"])\n",
    "subprocess.run([\"python\", \"-m\", \"onnxruntime.quantization.preprocess\", \"--input\", \"./onnx/decoder.onnx\", \"--output\", \"./onnx/decoder-infer.onnx\"])\n",
    "subprocess.run([\"python\", \"-m\", \"onnxruntime.quantization.preprocess\", \"--input\", \"./onnx/jointer.onnx\", \"--output\", \"./onnx/jointer-infer.onnx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_dynamic(\n",
    "    './onnx/encoder-infer.onnx', \n",
    "    './onnx/encoder-infer.quant.onnx',\n",
    "    weight_type=QuantType.QInt8,\n",
    "    op_types_to_quantize=['MatMul'])\n",
    "\n",
    "quantize_dynamic(\n",
    "    './onnx/decoder-infer.onnx', \n",
    "    './onnx/decoder-infer.quant.onnx',\n",
    "    weight_type=QuantType.QInt8,\n",
    "    op_types_to_quantize=['GRU'])\n",
    "\n",
    "quantize_dynamic(\n",
    "    './onnx/jointer-infer.onnx', \n",
    "    './onnx/jointer-infer.quant.onnx',\n",
    "    weight_type=QuantType.QInt8,\n",
    "    op_types_to_quantize=['MatMul'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_encoder_session = ort.InferenceSession(\"./onnx/encoder-infer.onnx\")\n",
    "ort_decoder_session = ort.InferenceSession(\"./onnx/decoder-infer.onnx\")\n",
    "ort_jointer_session = ort.InferenceSession(\"./onnx/jointer-infer.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_online_inference(audio, ort_encoder_session, ort_decoder_session, ort_jointer_session, tokenizer):\n",
    "    if type(audio) == torch.Tensor:\n",
    "        audio = audio.cpu().numpy()\n",
    "\n",
    "    if audio.ndim == 1:\n",
    "        audio = np.expand_dims(audio, 0)\n",
    "\n",
    "    audio_cache = np.zeros((1, N_FFT - HOP_LENGTH), dtype=np.float32)\n",
    "    conv1_cache = np.zeros((1, 80, 1), dtype=np.float32)\n",
    "    conv2_cache = np.zeros((1, 768, 1), dtype=np.float32)\n",
    "    conv3_cache = np.zeros((1, 768, 1), dtype=np.float32)\n",
    "\n",
    "    k_cache = np.zeros((12, 1, ATTENTION_CONTEXT_SIZE[0], 768), dtype=np.float32)\n",
    "    v_cache = np.zeros((12, 1, ATTENTION_CONTEXT_SIZE[0], 768), dtype=np.float32)\n",
    "    cache_len = np.zeros((1,), dtype=np.int32)\n",
    "\n",
    "    h_n = np.zeros((1, 1, 768), dtype=np.float32)\n",
    "    token = np.array([[RNNT_BLANK]], dtype=np.int64)\n",
    "\n",
    "    RTF = audio.shape[1] / SAMPLE_RATE\n",
    "    seq_ids = []\n",
    "\n",
    "    start = 0\n",
    "    for i in range(0, audio.shape[1], HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH)):\n",
    "        audio_chunk = audio[:, i:i+HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH)]\n",
    "        if audio_chunk.shape[1] < HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH):\n",
    "            audio_chunk = np.pad(audio_chunk, ((0, 0), (0, HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH) - audio_chunk.shape[1])))\n",
    "\n",
    "        r = ort_encoder_session.run(\n",
    "            None,\n",
    "            {\n",
    "                \"audio_chunk\": audio_chunk,\n",
    "                \"audio_cache.1\": audio_cache,\n",
    "                \"conv1_cache.1\": conv1_cache,\n",
    "                \"conv2_cache.1\": conv2_cache,\n",
    "                \"conv3_cache.1\": conv3_cache,\n",
    "                \"k_cache.1\": k_cache,\n",
    "                \"v_cache.1\": v_cache,\n",
    "                \"cache_len.1\": cache_len\n",
    "            }\n",
    "        )\n",
    "\n",
    "        enc_out, audio_cache, conv1_cache, conv2_cache, conv3_cache, k_cache, v_cache, cache_len = r\n",
    "\n",
    "        for time_idx in range(enc_out.shape[1]):\n",
    "            curent_seq_enc_out = enc_out[:, time_idx, :].reshape(1, 1, N_STATE)\n",
    "\n",
    "            not_blank = True\n",
    "            symbols_added = 0\n",
    "\n",
    "            while not_blank and symbols_added < 3:\n",
    "                dec, new_h_n = ort_decoder_session.run(\n",
    "                    None,\n",
    "                    {\n",
    "                        \"token\": token,\n",
    "                        \"h_n.1\": h_n\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                logits = ort_jointer_session.run(\n",
    "                    None,\n",
    "                    {\n",
    "                        \"enc\": curent_seq_enc_out,\n",
    "                        \"dec\": dec\n",
    "                    }\n",
    "                )[0]\n",
    "\n",
    "                new_token = int(logits.argmax())\n",
    "\n",
    "                if new_token == RNNT_BLANK:\n",
    "                    not_blank = False\n",
    "                else:\n",
    "                    symbols_added += 1\n",
    "                    token = np.array([[new_token]], dtype=np.int64)\n",
    "                    h_n = new_h_n\n",
    "                    seq_ids.append(new_token)\n",
    "    end = time.time()\n",
    "\n",
    "    return tokenizer.decode(seq_ids), RTF / (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = librosa.load(\"/path/to/vnbacnam.m4a\", sr=SAMPLE_RATE)[0]\n",
    "audio = np.pad(audio, (16000, 0)) # add some zeros to the start of the audio for warmup\n",
    "audio = np.expand_dims(audio, 0).astype(np.float32)\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_online_inference(audio, ort_encoder_session, ort_decoder_session, ort_jointer_session, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

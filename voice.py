import os
import torch
import torch.nn.functional as F
import sentencepiece as spm
import librosa
import numpy as np
import threading
import queue
import time
from collections import deque
import argparse
import pyaudio
import webrtcvad
from huggingface_hub import hf_hub_download
from tqdm import tqdm

from models.encoder import AudioEncoder
from models.decoder import Decoder
from models.jointer import Jointer

from constants import SAMPLE_RATE, N_FFT, HOP_LENGTH, N_MELS
from constants import RNNT_BLANK, PAD, VOCAB_SIZE, TOKENIZER_MODEL_PATH, MAX_SYMBOLS
from constants import ATTENTION_CONTEXT_SIZE
from constants import N_STATE, N_LAYER, N_HEAD


class VoiceActivityDetector:
    """Voice Activity Detector s·ª≠ d·ª•ng WebRTC VAD"""

    def __init__(self, aggressiveness=3, frame_duration_ms=30):
        self.vad = webrtcvad.Vad(aggressiveness)
        self.frame_duration_ms = frame_duration_ms
        self.sample_rate = SAMPLE_RATE
        self.frame_length = int(self.sample_rate * self.frame_duration_ms / 1000)

    def is_speech(self, audio_frame):
        """Ki·ªÉm tra frame c√≥ ch·ª©a gi·ªçng n√≥i kh√¥ng"""
        if audio_frame.dtype != np.int16:
            audio_frame = (audio_frame * 32767).astype(np.int16)

        if len(audio_frame) != self.frame_length:
            return False

        return self.vad.is_speech(audio_frame.tobytes(), self.sample_rate)


class ContinuousAudioBuffer:
    """Buffer li√™n t·ª•c cho streaming audio"""

    def __init__(self, chunk_duration=0.5, overlap_duration=0.1):
        """
        Args:
            chunk_duration: Th·ªùi gian m·ªói chunk ƒë·ªÉ process (gi√¢y) - ch·ªâ cho streaming mode
            overlap_duration: Th·ªùi gian overlap gi·ªØa c√°c chunk (gi√¢y) - ch·ªâ cho streaming mode
        """
        self.chunk_samples = int(chunk_duration * SAMPLE_RATE)
        self.overlap_samples = int(overlap_duration * SAMPLE_RATE)

        self.buffer = deque(maxlen=self.chunk_samples * 2)  # Buffer cho streaming mode

    def add_audio_streaming(self, audio_chunk):
        """Th√™m audio v√† return chunk cho streaming mode"""
        # Th√™m audio v√†o buffer
        for sample in audio_chunk:
            self.buffer.append(sample)

        # Ki·ªÉm tra xem ƒë√£ ƒë·ªß ƒë·ªÉ t·∫°o chunk m·ªõi ch∆∞a
        if len(self.buffer) >= self.chunk_samples:
            # L·∫•y chunk ƒë·ªÉ process
            chunk = np.array(list(self.buffer)[:self.chunk_samples])

            # Shift buffer ƒë·ªÉ chu·∫©n b·ªã cho chunk ti·∫øp theo (v·ªõi overlap)
            shift_amount = self.chunk_samples - self.overlap_samples
            for _ in range(shift_amount):
                if len(self.buffer) > self.overlap_samples:
                    self.buffer.popleft()

            return chunk

        return None


class VADSegmentBuffer:
    """Buffer s·ª≠ d·ª•ng VAD ƒë·ªÉ c·∫Øt segment cho offline mode"""

    def __init__(self, max_duration=30.0, silence_duration=1.5, min_duration=0.5):
        """
        Args:
            max_duration: Th·ªùi gian t·ªëi ƒëa c·ªßa m·ªôt segment (gi√¢y)
            silence_duration: Th·ªùi gian im l·∫∑ng ƒë·ªÉ k·∫øt th√∫c segment (gi√¢y)
            min_duration: Th·ªùi gian t·ªëi thi·ªÉu ƒë·ªÉ coi l√† segment h·ª£p l·ªá (gi√¢y)
        """
        self.max_samples = int(max_duration * SAMPLE_RATE)
        self.silence_samples = int(silence_duration * SAMPLE_RATE)
        self.min_samples = int(min_duration * SAMPLE_RATE)

        self.buffer = deque()
        self.silence_counter = 0
        self.has_speech = False

        self.vad = VoiceActivityDetector(aggressiveness=2)

    def add_audio(self, audio_chunk):
        """
        Th√™m audio chunk v√† ki·ªÉm tra xem c√≥ segment ho√†n ch·ªânh kh√¥ng

        Returns:
            tuple: (has_complete_segment, audio_segment)
        """
        # Th√™m audio v√†o buffer
        for sample in audio_chunk:
            self.buffer.append(sample)

        # Ki·ªÉm tra buffer kh√¥ng qu√° d√†i
        while len(self.buffer) > self.max_samples:
            self.buffer.popleft()

        # Ph√¢n t√≠ch VAD cho chunk n√†y
        chunk_has_speech = self._analyze_vad(audio_chunk)

        if chunk_has_speech:
            self.has_speech = True
            self.silence_counter = 0
        else:
            self.silence_counter += len(audio_chunk)

        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán ƒë·ªÉ tr·∫£ v·ªÅ segment
        if self.has_speech and self.silence_counter >= self.silence_samples:
            # ƒê√£ c√≥ speech v√† im l·∫∑ng ƒë·ªß l√¢u
            if len(self.buffer) >= self.min_samples:
                # Segment ƒë·ªß d√†i, tr·∫£ v·ªÅ
                audio_segment = np.array(list(self.buffer))
                self._reset()
                return True, audio_segment

        # Ki·ªÉm tra buffer qu√° d√†i (force return)
        if len(self.buffer) >= self.max_samples and self.has_speech:
            audio_segment = np.array(list(self.buffer))
            self._reset()
            return True, audio_segment

        return False, None

    def _analyze_vad(self, audio_chunk):
        """Ph√¢n t√≠ch VAD cho audio chunk"""
        frame_size = self.vad.frame_length
        voice_frames = 0
        total_frames = 0

        for i in range(0, len(audio_chunk), frame_size):
            frame = audio_chunk[i:i + frame_size]
            if len(frame) == frame_size:
                total_frames += 1
                if self.vad.is_speech(frame):
                    voice_frames += 1

        # Coi l√† c√≥ speech n·∫øu > 30% frames c√≥ voice
        return (voice_frames / max(total_frames, 1)) > 0.3 if total_frames > 0 else False

    def _reset(self):
        """Reset buffer sau khi tr·∫£ v·ªÅ segment"""
        self.buffer.clear()
        self.silence_counter = 0
        self.has_speech = False

    def get_current_audio(self):
        """L·∫•y audio hi·ªán t·∫°i trong buffer"""
        return np.array(list(self.buffer))


class StreamingMicrophone:
    """Microphone streaming li√™n t·ª•c"""

    def __init__(self, chunk_size=800):  # 50ms chunks
        self.chunk_size = chunk_size
        self.sample_rate = SAMPLE_RATE
        self.channels = 1
        self.format = pyaudio.paFloat32

        self.audio = pyaudio.PyAudio()
        self.stream = None
        self.audio_queue = queue.Queue()
        self.is_recording = False

        # VAD ƒë·ªÉ detect voice activity
        self.vad = VoiceActivityDetector(aggressiveness=2)

    def start_stream(self):
        """B·∫Øt ƒë·∫ßu stream li√™n t·ª•c"""
        self.stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.sample_rate,
            input=True,
            frames_per_buffer=self.chunk_size,
            stream_callback=self._audio_callback
        )
        self.is_recording = True
        self.stream.start_stream()

    def stop_stream(self):
        """D·ª´ng stream"""
        self.is_recording = False
        if self.stream:
            self.stream.stop_stream()
            self.stream.close()
        self.audio.terminate()

    def _audio_callback(self, in_data, frame_count, time_info, status):
        """Callback x·ª≠ l√Ω audio li√™n t·ª•c"""
        if self.is_recording:
            audio_data = np.frombuffer(in_data, dtype=np.float32)
            self.audio_queue.put(audio_data)
        return (None, pyaudio.paContinue)

    def get_audio_chunk(self):
        """L·∫•y audio chunk t·ª´ queue"""
        try:
            return self.audio_queue.get_nowait()
        except queue.Empty:
            return None

    def has_voice_activity(self, audio_chunk):
        """Ki·ªÉm tra c√≥ voice activity kh√¥ng"""
        frame_size = self.vad.frame_length
        voice_frames = 0
        total_frames = 0

        for i in range(0, len(audio_chunk), frame_size):
            frame = audio_chunk[i:i + frame_size]
            if len(frame) == frame_size:
                total_frames += 1
                if self.vad.is_speech(frame):
                    voice_frames += 1

        return voice_frames / max(total_frames, 1) > 0.3 if total_frames > 0 else False


class StreamingASR:
    """ASR v·ªõi streaming mode v√† continuous recording"""

    def __init__(self, model_path=None, tokenizer_path=None, device=None,
                 streaming_mode=True, chunk_duration=0.5, append_mode=True,
                 silence_duration=1.5):
        """
        Args:
            streaming_mode: True ƒë·ªÉ d√πng online streaming, False ƒë·ªÉ d√πng offline mode v·ªõi VAD
            chunk_duration: Th·ªùi gian m·ªói chunk ƒë·ªÉ process (gi√¢y) - ch·ªâ cho streaming mode
            append_mode: True ƒë·ªÉ append text, False ƒë·ªÉ replace text
            silence_duration: Th·ªùi gian im l·∫∑ng ƒë·ªÉ k·∫øt th√∫c segment (gi√¢y) - ch·ªâ cho offline mode
        """
        # Setup device
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
        print(f"üîß Using device: {self.device}")

        # Load tokenizer
        if tokenizer_path is None:
            tokenizer_path = TOKENIZER_MODEL_PATH
        self.tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_path)

        # Load model
        print("üì• Loading ASR model...")
        self._load_model(model_path)
        self._load_mel_filters()

        # Setup streaming components
        self.streaming_mode = streaming_mode
        self.append_mode = append_mode
        self.microphone = StreamingMicrophone()

        if self.streaming_mode:
            # Streaming mode: s·ª≠ d·ª•ng buffer theo th·ªùi gian
            self.audio_buffer = ContinuousAudioBuffer(chunk_duration=chunk_duration)
        else:
            # Offline mode: s·ª≠ d·ª•ng VAD buffer
            self.vad_buffer = VADSegmentBuffer(
                max_duration=30.0,
                silence_duration=silence_duration,
                min_duration=0.5
            )

        # Initialize streaming states n·∫øu d√πng streaming mode
        if self.streaming_mode:
            self._init_streaming_states()

        self.is_running = False
        self.transcription_queue = queue.Queue()

    def _load_mel_filters(self):
        """Load mel filterbank matrix"""
        try:
            self.mel_filters_80 = np.load("./utils/mel_filters.npz", allow_pickle=False)["mel_80"]
            self.mel_filters_80 = torch.from_numpy(self.mel_filters_80)
        except FileNotFoundError:
            print("‚ö†Ô∏è  Creating mel filters from librosa...")
            self.mel_filters_80 = librosa.filters.mel(sr=SAMPLE_RATE, n_fft=N_FFT, n_mels=N_MELS)
            self.mel_filters_80 = torch.from_numpy(self.mel_filters_80)
            os.makedirs("./utils", exist_ok=True)
            np.savez_compressed("./utils/mel_filters.npz", mel_80=self.mel_filters_80.numpy())

    def _load_model(self, model_path):
        """Load model t·ª´ checkpoint"""
        if model_path is None:
            print("üì• Downloading model from huggingface hub...")
            model_path = hf_hub_download(
                repo_id="hkab/vietnamese-asr-model",
                filename="rnnt-latest.ckpt",
                subfolder="rnnt-whisper-small/80_3"
            )

        print(f"üìÇ Loading model from {model_path}")
        checkpoint = torch.load(model_path, map_location="cpu", weights_only=True)

        # Split weights
        encoder_weight = {}
        decoder_weight = {}
        joint_weight = {}

        for k, v in checkpoint['state_dict'].items():
            if 'alibi' in k:
                continue
            if 'encoder' in k:
                encoder_weight[k.replace('encoder.', '')] = v
            elif 'decoder' in k:
                decoder_weight[k.replace('decoder.', '')] = v
            elif 'joint' in k:
                joint_weight[k.replace('joint.', '')] = v

        # Initialize model components
        self.encoder = AudioEncoder(
            N_MELS,
            n_state=N_STATE,
            n_head=N_HEAD,
            n_layer=N_LAYER,
            att_context_size=ATTENTION_CONTEXT_SIZE
        )
        self.decoder = Decoder(vocab_size=VOCAB_SIZE + 1)
        self.joint = Jointer(vocab_size=VOCAB_SIZE + 1)

        # Load weights
        self.encoder.load_state_dict(encoder_weight, strict=False)
        self.decoder.load_state_dict(decoder_weight, strict=False)
        self.joint.load_state_dict(joint_weight, strict=False)

        # Move to device and eval mode
        self.encoder = self.encoder.to(self.device).eval()
        self.decoder = self.decoder.to(self.device).eval()
        self.joint = self.joint.to(self.device).eval()

    def _init_streaming_states(self):
        """Initialize streaming states cho online mode"""
        self.audio_cache = torch.zeros(240, device=self.device)
        self.conv1_cache = torch.zeros(1, 80, 1, device=self.device)
        self.conv2_cache = torch.zeros(1, 768, 1, device=self.device)
        self.conv3_cache = torch.zeros(1, 768, 1, device=self.device)

        self.k_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=self.device)
        self.v_cache = torch.zeros(12, 1, ATTENTION_CONTEXT_SIZE[0], 768, device=self.device)
        self.cache_len = torch.zeros(1, dtype=torch.int, device=self.device)

        self.hypothesis = [[None, None]]  # [label, state]
        self.seq_ids = []

    def log_mel_spectrogram(self, audio, n_mels=N_MELS, padding=0, streaming=False):
        """Compute log-Mel spectrogram"""
        audio = audio.to(self.device)

        if padding > 0:
            audio = F.pad(audio, (0, padding))

        window = torch.hann_window(N_FFT).to(audio.device)

        if not streaming:
            stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)
        else:
            stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, center=False, return_complex=True)

        magnitudes = stft[..., :-1].abs() ** 2
        mel_filters_device = self.mel_filters_80.to(audio.device)
        mel_spec = mel_filters_device @ magnitudes

        log_spec = torch.clamp(mel_spec, min=1e-10).log10()
        log_spec = (log_spec + 4.0) / 4.0

        return log_spec

    @torch.no_grad()
    def _process_audio_chunk_offline(self, audio_chunk):
        """Process audio chunk using offline mode"""
        if len(audio_chunk) < 1600:
            return ""

        audio_tensor = torch.from_numpy(audio_chunk.astype(np.float32))

        # Compute mel spectrogram
        mels = self.log_mel_spectrogram(audio=audio_tensor, n_mels=N_MELS, padding=0, streaming=False)
        x = mels.reshape(1, *mels.shape)
        x_len = torch.tensor([x.shape[2]]).to(self.device)

        # Encoder forward
        enc_out, _ = self.encoder(x, x_len)

        # Greedy decoding
        hypothesis = [[None, None]]
        seq_enc_out = enc_out[0, :, :].unsqueeze(0)
        seq_ids = []

        for time_idx in range(seq_enc_out.shape[1]):
            current_seq_enc_out = seq_enc_out[:, time_idx, :].unsqueeze(1)
            not_blank = True
            symbols_added = 0

            while not_blank and symbols_added < MAX_SYMBOLS:
                if hypothesis[-1][0] is None:
                    last_token = torch.tensor([[RNNT_BLANK]], dtype=torch.long, device=self.device)
                    last_seq_h_n = None
                else:
                    last_token = hypothesis[-1][0]
                    last_seq_h_n = hypothesis[-1][1]

                if last_seq_h_n is None:
                    current_seq_dec_out, current_seq_h_n = self.decoder(last_token)
                else:
                    current_seq_dec_out, current_seq_h_n = self.decoder(last_token, last_seq_h_n)

                logits = self.joint(current_seq_enc_out, current_seq_dec_out)[0, 0, 0, :]
                _, token_id = logits.max(0)
                token_id = token_id.detach().item()

                if token_id == RNNT_BLANK:
                    not_blank = False
                else:
                    symbols_added += 1
                    hypothesis.append([
                        torch.tensor([[token_id]], dtype=torch.long, device=self.device),
                        current_seq_h_n
                    ])
                    seq_ids.append(token_id)

        return self.tokenizer.decode(seq_ids)

    @torch.no_grad()
    def _process_audio_chunk_streaming(self, audio_chunk):
        """Process audio chunk using streaming mode"""
        chunk_size = HOP_LENGTH * 31 + N_FFT - (N_FFT - HOP_LENGTH)
        audio_tensor = torch.from_numpy(audio_chunk.astype(np.float32)).to(self.device)

        # Prepare audio chunk with cache
        audio_chunk_with_cache = torch.cat([self.audio_cache, audio_tensor])

        if audio_chunk_with_cache.shape[0] < chunk_size + N_FFT - HOP_LENGTH:
            audio_chunk_with_cache = F.pad(
                audio_chunk_with_cache,
                (0, chunk_size + N_FFT - HOP_LENGTH - audio_chunk_with_cache.shape[0])
            )

        # Update audio cache
        self.audio_cache = audio_chunk_with_cache[-(N_FFT - HOP_LENGTH):]

        # Compute mel spectrogram
        x_chunk = self.log_mel_spectrogram(
            audio=audio_chunk_with_cache,
            n_mels=N_MELS,
            padding=0,
            streaming=True
        )
        x_chunk = x_chunk.reshape(1, *x_chunk.shape)

        if x_chunk.shape[-1] < 32:
            x_chunk = F.pad(x_chunk, (0, 32 - x_chunk.shape[-1]))

        # Encoder forward v·ªõi caching
        x_chunk = torch.cat([self.conv1_cache, x_chunk], dim=2)
        self.conv1_cache = x_chunk[:, :, -1].unsqueeze(2)
        x_chunk = F.gelu(self.encoder.conv1(x_chunk))

        x_chunk = torch.cat([self.conv2_cache, x_chunk], dim=2)
        self.conv2_cache = x_chunk[:, :, -1].unsqueeze(2)
        x_chunk = F.gelu(self.encoder.conv2(x_chunk))

        x_chunk = torch.cat([self.conv3_cache, x_chunk], dim=2)
        self.conv3_cache = x_chunk[:, :, -1].unsqueeze(2)
        x_chunk = F.gelu(self.encoder.conv3(x_chunk))

        x_chunk = x_chunk.permute(0, 2, 1)

        # Attention v·ªõi caching
        x_len = torch.tensor([x_chunk.shape[1]]).to(self.device)
        if self.k_cache is not None:
            x_len = x_len + ATTENTION_CONTEXT_SIZE[0]
            offset = torch.neg(self.cache_len) + ATTENTION_CONTEXT_SIZE[0]
        else:
            offset = None

        attn_mask = self.encoder.form_attention_mask_for_streaming(
            self.encoder.att_context_size, x_len, offset.to(self.device), self.device
        )

        if self.k_cache is not None:
            attn_mask = attn_mask[:, :, ATTENTION_CONTEXT_SIZE[0]:, :]

        # Process through transformer blocks
        new_k_cache = []
        new_v_cache = []
        for i, block in enumerate(self.encoder.blocks):
            x_chunk, layer_k_cache, layer_v_cache = block(
                x_chunk, mask=attn_mask, k_cache=self.k_cache[i], v_cache=self.v_cache[i]
            )
            new_k_cache.append(layer_k_cache)
            new_v_cache.append(layer_v_cache)

        enc_out = self.encoder.ln_post(x_chunk)

        # Update caches
        self.k_cache = torch.stack(new_k_cache, dim=0)
        self.v_cache = torch.stack(new_v_cache, dim=0)
        self.cache_len = torch.clamp(self.cache_len + ATTENTION_CONTEXT_SIZE[-1] + 1, max=ATTENTION_CONTEXT_SIZE[0])

        # Greedy decoding
        seq_enc_out = enc_out[0, :, :].unsqueeze(0)
        new_tokens = []

        for time_idx in range(seq_enc_out.shape[1]):
            current_seq_enc_out = seq_enc_out[:, time_idx, :].unsqueeze(1)
            not_blank = True
            symbols_added = 0

            while not_blank and symbols_added < MAX_SYMBOLS:
                if self.hypothesis[-1][0] is None:
                    last_token = torch.tensor([[RNNT_BLANK]], dtype=torch.long, device=self.device)
                    last_seq_h_n = None
                else:
                    last_token = self.hypothesis[-1][0]
                    last_seq_h_n = self.hypothesis[-1][1]

                if last_seq_h_n is None:
                    current_seq_dec_out, current_seq_h_n = self.decoder(last_token)
                else:
                    current_seq_dec_out, current_seq_h_n = self.decoder(last_token, last_seq_h_n)

                logits = self.joint(current_seq_enc_out, current_seq_dec_out)[0, 0, 0, :]
                _, token_id = logits.max(0)
                token_id = token_id.detach().item()

                if token_id == RNNT_BLANK:
                    not_blank = False
                else:
                    symbols_added += 1
                    self.hypothesis.append([
                        torch.tensor([[token_id]], dtype=torch.long, device=self.device),
                        current_seq_h_n
                    ])
                    self.seq_ids.append(token_id)
                    new_tokens.append(token_id)

        # Return partial transcription n·∫øu c√≥ tokens m·ªõi
        if new_tokens:
            return self.tokenizer.decode(new_tokens)
        return ""

    def _audio_processing_thread(self):
        """Thread x·ª≠ l√Ω audio li√™n t·ª•c"""
        print("üéØ Audio processing thread started")

        while self.is_running:
            # L·∫•y audio chunk t·ª´ microphone
            audio_chunk = self.microphone.get_audio_chunk()

            if audio_chunk is not None:
                if self.streaming_mode:
                    # Streaming mode: x·ª≠ l√Ω theo chunk th·ªùi gian c·ªë ƒë·ªãnh
                    chunk_to_process = self.audio_buffer.add_audio_streaming(audio_chunk)

                    if chunk_to_process is not None:
                        # Ki·ªÉm tra voice activity tr∆∞·ªõc khi process
                        if self.microphone.has_voice_activity(chunk_to_process):
                            partial_text = self._process_audio_chunk_streaming(chunk_to_process)
                            if partial_text.strip():
                                self.transcription_queue.put(('partial', partial_text))
                else:
                    # Offline mode: s·ª≠ d·ª•ng VAD ƒë·ªÉ c·∫Øt segment
                    has_segment, segment_to_process = self.vad_buffer.add_audio(audio_chunk)

                    if has_segment and segment_to_process is not None:
                        print(
                            f"\nüéØ VAD detected complete segment ({len(segment_to_process) / SAMPLE_RATE:.1f}s), transcribing...")
                        partial_text = self._process_audio_chunk_offline(segment_to_process)
                        if partial_text.strip():
                            self.transcription_queue.put(('partial', partial_text))
                        else:
                            print("üîá No speech detected in this segment")

            time.sleep(0.01)  # Small delay

    def _output_thread(self):
        """Thread ƒë·ªÉ output k·∫øt qu·∫£ li√™n t·ª•c"""
        print("üìù Output thread started")
        current_text = ""
        all_transcriptions = []

        while self.is_running:
            try:
                msg_type, text = self.transcription_queue.get(timeout=0.1)

                if msg_type == 'partial':
                    if self.streaming_mode:
                        # Streaming mode: always append new text
                        current_text += text
                        print(f"\rüîä {current_text}", end="", flush=True)
                    else:
                        # Offline mode: check append_mode
                        if text.strip():
                            if self.append_mode:
                                # Append mode: t√≠ch l≈©y t·∫•t c·∫£ text
                                all_transcriptions.append(text.strip())
                                current_text = " ".join(all_transcriptions)
                            else:
                                # Replace mode: ch·ªâ hi·ªÉn th·ªã text hi·ªán t·∫°i
                                current_text = text.strip()
                            print(f"\rüîä {current_text}", end="", flush=True)

                elif msg_type == 'final':
                    current_text = text
                    print(f"\n‚úÖ Final: {current_text}")
                    if self.streaming_mode:
                        # Reset cho streaming mode
                        self._init_streaming_states()
                    current_text = ""
                    all_transcriptions = []

                elif msg_type == 'reset':
                    print(f"\nüîÑ Transcription reset!")
                    current_text = ""
                    all_transcriptions = []
                    if self.streaming_mode:
                        self._init_streaming_states()
                    print("üîä ", end="", flush=True)

            except queue.Empty:
                continue

    def start_continuous_recognition(self):
        """B·∫Øt ƒë·∫ßu continuous recognition"""
        if self.streaming_mode:
            mode_desc = f"Streaming (chunks: {self.audio_buffer.chunk_samples / SAMPLE_RATE:.1f}s)"
        else:
            mode_desc = f"Offline VAD (silence: {self.vad_buffer.silence_samples / SAMPLE_RATE:.1f}s)"

        print(f"üöÄ Starting continuous recognition...")
        print(f"üìä Mode: {mode_desc}")
        print(f"üìù Output: {'Append' if self.append_mode else 'Replace'} mode")
        print("üí° Speak continuously. Press Ctrl+C to stop.")
        print("üîÑ Press Enter during recognition to reset current transcription.")
        print("=" * 60)

        self.is_running = True
        self.microphone.start_stream()

        # Start threads
        audio_thread = threading.Thread(target=self._audio_processing_thread)
        output_thread = threading.Thread(target=self._output_thread)
        input_thread = threading.Thread(target=self._input_monitor_thread)

        audio_thread.daemon = True
        output_thread.daemon = True
        input_thread.daemon = True

        audio_thread.start()
        output_thread.start()
        input_thread.start()

        try:
            while self.is_running:
                time.sleep(0.1)
        except KeyboardInterrupt:
            print("\nüõë Stopping recognition...")
        finally:
            self.stop_recognition()

    def _input_monitor_thread(self):
        """Thread ƒë·ªÉ monitor input t·ª´ user"""
        import sys
        import select

        while self.is_running:
            # Ki·ªÉm tra c√≥ input kh√¥ng (ch·ªâ works tr√™n Unix-like systems)
            try:
                if sys.stdin in select.select([sys.stdin], [], [], 0.1)[0]:
                    input()  # Clear the input
                    self.transcription_queue.put(('reset', ''))
            except:
                # Fallback cho Windows - kh√¥ng c√≥ select
                time.sleep(0.1)
                pass

    def stop_recognition(self):
        """D·ª´ng recognition"""
        self.is_running = False
        self.microphone.stop_stream()
        print("‚úÖ Recognition stopped.")

        # Output final transcription n·∫øu c√≥
        if self.streaming_mode and self.seq_ids:
            final_text = self.tokenizer.decode(self.seq_ids)
            print(f"\nüéØ Final transcription: {final_text}")


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Continuous Vietnamese ASR with Microphone")

    parser.add_argument(
        "--model", type=str, default=None,
        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model checkpoint"
    )

    parser.add_argument(
        "--tokenizer", type=str, default=None,
        help="ƒê∆∞·ªùng d·∫´n ƒë·∫øn tokenizer model"
    )

    parser.add_argument(
        "--device", type=str, default=None,
        help="Device ƒë·ªÉ ch·∫°y model (cuda/cpu)"
    )

    parser.add_argument(
        "--streaming", action="store_true",
        help="S·ª≠ d·ª•ng streaming mode (online), m·∫∑c ƒë·ªãnh l√† offline"
    )

    parser.add_argument(
        "--chunk-duration", type=float, default=0.5,
        help="Th·ªùi gian m·ªói chunk ƒë·ªÉ process cho streaming mode (gi√¢y)"
    )

    parser.add_argument(
        "--silence-duration", type=float, default=1.5,
        help="Th·ªùi gian im l·∫∑ng ƒë·ªÉ k·∫øt th√∫c segment cho offline mode (gi√¢y)"
    )

    parser.add_argument(
        "--no-append", action="store_true",
        help="Kh√¥ng append text, thay v√†o ƒë√≥ replace text m·ªói chunk"
    )

    args = parser.parse_args()

    # Initialize StreamingASR
    asr = StreamingASR(
        model_path=args.model,
        tokenizer_path=args.tokenizer,
        device=args.device,
        streaming_mode=args.streaming,
        chunk_duration=args.chunk_duration,
        append_mode=not args.no_append,
        silence_duration=args.silence_duration
    )

    # Start continuous recognition
    asr.start_continuous_recognition()


if __name__ == "__main__":
    main()
# üöÄ Training Improvements for Whisper RNN-T

## üìä **Current Analysis**

### ‚úÖ **Strengths:**
- Step-based model saving with WER monitoring
- Good augmentation pipeline
- Proper RNN-T loss implementation
- Streaming-compatible architecture

### ‚ö†Ô∏è **Areas for Improvement:**

## üéØ **1. Learning Rate & Optimization**

### **Current Issues:**
- Fixed LR schedule may not be optimal
- No gradient clipping
- Simple AdamW without advanced techniques

### **Improvements:**
```yaml
# Enhanced optimizer config
training:
  optimizer:
    lr: 5e-5  # Lower initial LR for fine-tuning
    min_lr: 1e-6  # Lower minimum
    betas: [0.9, 0.999]  # Standard Adam betas
    eps: 1e-8
    weight_decay: 0.01  # Add weight decay
    
  # Gradient clipping
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  
  # Advanced scheduler
  scheduler:
    type: "cosine_annealing"  # Better than linear
    warmup_steps: 5000  # More warmup
    total_steps: 100000  # Adjust based on data
    min_lr_ratio: 0.1
```

## üéµ **2. Audio Processing Improvements**

### **Current Issues:**
- Fixed mel spectrogram normalization
- No SpecAugment
- Limited frequency masking

### **Improvements:**
```python
# Add SpecAugment to dataset.py
from audiomentations import SpecAugment

# In AudioDataset.__init__():
if augment:
    augmentations.extend([
        # SpecAugment for better generalization
        SpecAugment(
            time_mask_param=40,
            freq_mask_param=15,
            num_time_masks=2,
            num_freq_masks=2,
            p=0.5
        ),
        
        # Speed perturbation
        TimeStretch(
            min_rate=0.85, max_rate=1.15,
            leave_length_unchanged=False, p=0.3
        )
    ])
```

## üß† **3. Model Architecture Enhancements**

### **Decoder Improvements:**
```python
# Enhanced decoder with dropout and layer norm
class ImprovedDecoder(nn.Module):
    def __init__(self, vocab_size=1024, embed_dim=512, hidden_dim=512, 
                 num_layers=2, dropout=0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.dropout = nn.Dropout(dropout)
        
        # Multi-layer LSTM with dropout
        self.rnn = nn.LSTM(
            embed_dim, hidden_dim, 
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True
        )
        
        self.layer_norm = nn.LayerNorm(hidden_dim)
    
    def forward(self, y, hidden=None):
        y = self.embedding(y)
        y = self.dropout(y)
        
        if hidden is not None:
            y, hidden = self.rnn(y, hidden)
        else:
            # Better initialization
            h_0 = torch.zeros(self.num_layers, y.size(0), self.hidden_dim, device=y.device)
            c_0 = torch.zeros(self.num_layers, y.size(0), self.hidden_dim, device=y.device)
            y, hidden = self.rnn(y, (h_0, c_0))
        
        y = self.layer_norm(y)
        return y, hidden
```

## üìà **4. Training Strategy Improvements**

### **Curriculum Learning:**
```python
# Add to train.py
class CurriculumScheduler:
    def __init__(self, start_max_duration=5.0, end_max_duration=15.0, total_steps=50000):
        self.start_max = start_max_duration
        self.end_max = end_max_duration
        self.total_steps = total_steps
    
    def get_max_duration(self, current_step):
        progress = min(current_step / self.total_steps, 1.0)
        return self.start_max + (self.end_max - self.start_max) * progress
```

### **Label Smoothing:**
```python
# Add to training_step
class LabelSmoothingRNNTLoss(nn.Module):
    def __init__(self, blank_id, smoothing=0.1):
        super().__init__()
        self.blank_id = blank_id
        self.smoothing = smoothing
        self.rnnt_loss = warprnnt_numba.RNNTLossNumba(blank=blank_id, reduction="mean")
    
    def forward(self, logits, targets, input_lengths, target_lengths):
        # Apply label smoothing
        if self.smoothing > 0:
            vocab_size = logits.size(-1)
            smooth_targets = torch.full_like(logits, self.smoothing / (vocab_size - 1))
            smooth_targets.scatter_(-1, targets.unsqueeze(-1), 1.0 - self.smoothing)
            
        return self.rnnt_loss(logits, targets, input_lengths, target_lengths)
```

## üîÑ **5. Data Loading Optimizations**

### **Dynamic Batching:**
```python
# Improved collate function with dynamic batching
def dynamic_collate_fn(batch, max_tokens=8000):
    # Sort by sequence length
    batch = sorted(batch, key=lambda x: x[0].shape[-1])
    
    # Dynamic batching based on total tokens
    batches = []
    current_batch = []
    current_tokens = 0
    
    for item in batch:
        tokens = item[0].shape[-1]
        if current_tokens + tokens > max_tokens and current_batch:
            batches.append(current_batch)
            current_batch = [item]
            current_tokens = tokens
        else:
            current_batch.append(item)
            current_tokens += tokens
    
    if current_batch:
        batches.append(current_batch)
    
    return [collate_fn(b) for b in batches]
```

## üìä **6. Monitoring & Debugging**

### **Enhanced Metrics:**
```python
# Add to validation_step
def validation_step(self, batch, batch_idx):
    # ... existing code ...
    
    # Additional metrics
    self.log("val_loss", loss.item(), prog_bar=True)
    self.log("val_wer", all_wer, prog_bar=True)
    
    # Character-level metrics
    cer = character_error_rate(all_true, all_pred)
    self.log("val_cer", cer, prog_bar=True)
    
    # Length statistics
    pred_lengths = [len(p.split()) for p in all_pred]
    true_lengths = [len(t.split()) for t in all_true]
    self.log("avg_pred_length", np.mean(pred_lengths))
    self.log("avg_true_length", np.mean(true_lengths))
    
    return loss
```

## üéõÔ∏è **7. Hyperparameter Tuning**

### **Recommended Settings:**
```yaml
# Optimized config
training:
  batch_size: 16  # Reduce for better gradients
  accumulate_grad_batches: 2  # Effective batch size = 32
  
  optimizer:
    lr: 3e-5  # Lower for stability
    weight_decay: 0.01
    
  scheduler:
    warmup_steps: 8000  # More warmup
    
model:
  decoder:
    num_layers: 2  # Deeper decoder
    dropout: 0.1
    
  joint:
    dropout: 0.1  # Add dropout to joint network

# Better augmentation balance
augmentation:
  gain:
    prob: 0.6  # Reduce aggressive augmentation
  pitch_shift:
    prob: 0.2
  background_noise:
    prob: 0.2
    min_snr_db: 15.0  # Higher SNR for cleaner training
```

## üöÄ **8. Advanced Techniques**

### **Exponential Moving Average:**
```python
# Add EMA for better convergence
from torch_ema import ExponentialMovingAverage

class StreamingRNNT(pl.LightningModule):
    def __init__(self, config):
        super().__init__()
        # ... existing code ...
        
        # EMA for stable training
        self.ema = ExponentialMovingAverage(self.parameters(), decay=0.999)
    
    def training_step(self, batch, batch_idx):
        loss = # ... compute loss ...
        
        # Update EMA
        if self.global_step % 10 == 0:
            self.ema.update()
        
        return loss
```

## üìã **Implementation Priority:**

1. **High Priority:**
   - Add gradient clipping
   - Implement SpecAugment
   - Improve learning rate schedule
   - Add label smoothing

2. **Medium Priority:**
   - Enhanced decoder architecture
   - Better monitoring metrics
   - Dynamic batching

3. **Low Priority:**
   - EMA training
   - Curriculum learning
   - Advanced augmentations

These improvements should significantly boost training stability and final model performance!
